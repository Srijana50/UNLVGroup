{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"This little kitty came to play when I was eating at a restaurant.\",\n",
    "             \"Merley has the best squooshy kitten belly.\",\n",
    "             \"Google Translate app is incredible.\",\n",
    "             \"If you open 100 tab in google you get a smiley face.\",\n",
    "             \"Best cat photo I've ever taken.\",\n",
    "             \"Climbing ninja cat.\",\n",
    "             \"Impressed with google map feedback.\",\n",
    "             \"Key promoter extension for Google Chrome.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "       n_clusters=2, n_init=1, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_k = 2\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " google\n",
      " feedback\n",
      " map\n",
      " app\n",
      " impressed\n",
      " incredible\n",
      " translate\n",
      " key\n",
      " extension\n",
      " chrome\n",
      "Cluster 1:\n",
      " cat\n",
      " best\n",
      " climbing\n",
      " ninja\n",
      " ve\n",
      " photo\n",
      " taken\n",
      " belly\n",
      " merley\n",
      " kitten\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind]),\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction\")\n",
    "Y = vectorizer.transform([\"chrome browser to open.\"])\n",
    "prediction = model.predict(Y)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction\")\n",
    "Y = vectorizer.transform([\"My cat is hungry.\"])\n",
    "prediction = model.predict(Y)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'little', 'kitty', 'came', 'to', 'play', 'when', 'i', 'was', 'eating', 'at', 'a', 'restaurant'], ['merley', 'has', 'the', 'best', 'squooshy', 'kitten', 'belly'], ['google', 'translate', 'app', 'is', 'incredible'], ['if', 'you', 'open', '100', 'tab', 'in', 'google', 'you', 'get', 'a', 'smiley', 'face'], ['best', 'cat', 'photo', 'ive', 'ever', 'taken'], ['climbing', 'ninja', 'cat'], ['impressed', 'with', 'google', 'map', 'feedback'], ['key', 'promoter', 'extension', 'for', 'google', 'chrome']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "plainDoc = []\n",
    "lcDoc = []\n",
    "tokens = []\n",
    "\n",
    "for doc in documents:\n",
    "    doc = re.sub(\"[^\\w\\s]\", \"\", doc)\n",
    "    plainDoc.append(doc)\n",
    "\n",
    "for doc in plainDoc:\n",
    "    doc = doc.lower()\n",
    "    lcDoc.append(doc)\n",
    "    \n",
    "for doc in lcDoc:\n",
    "    words = word_tokenize(doc)\n",
    "    tokens.append(words)\n",
    "    \n",
    "print (tokens)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 a\n",
      "1 at\n",
      "2 came\n",
      "3 eating\n",
      "4 i\n",
      "5 kitty\n",
      "6 little\n",
      "7 play\n",
      "8 restaurant\n",
      "9 this\n",
      "10 to\n",
      "11 was\n",
      "12 when\n",
      "13 belly\n",
      "14 best\n",
      "15 has\n",
      "16 kitten\n",
      "17 merley\n",
      "18 squooshy\n",
      "19 the\n",
      "20 app\n",
      "21 google\n",
      "22 incredible\n",
      "23 is\n",
      "24 translate\n",
      "25 100\n",
      "26 face\n",
      "27 get\n",
      "28 if\n",
      "29 in\n",
      "30 open\n",
      "31 smiley\n",
      "32 tab\n",
      "33 you\n",
      "34 cat\n",
      "35 ever\n",
      "36 ive\n",
      "37 photo\n",
      "38 taken\n",
      "39 climbing\n",
      "40 ninja\n",
      "41 feedback\n",
      "42 impressed\n",
      "43 map\n",
      "44 with\n",
      "45 chrome\n",
      "46 extension\n",
      "47 for\n",
      "48 key\n",
      "49 promoter\n"
     ]
    }
   ],
   "source": [
    "id2word = gensim.corpora.Dictionary(tokens)\n",
    "count = 0\n",
    "for k, v in id2word.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0 is \"a\"\n",
      "Word 1 is \"at\"\n",
      "Word 2 is \"came\"\n",
      "Word 3 is \"eating\"\n",
      "Word 4 is \"i\"\n",
      "Word 5 is \"kitty\"\n",
      "Word 6 is \"little\"\n",
      "Word 7 is \"play\"\n",
      "Word 8 is \"restaurant\"\n",
      "Word 9 is \"this\"\n",
      "Word 10 is \"to\"\n",
      "Word 11 is \"was\"\n",
      "Word 12 is \"when\"\n"
     ]
    }
   ],
   "source": [
    "bow_corpus = [id2word.doc2bow(doc) for doc in tokens]\n",
    "for i in range(len(bow_corpus[0])):\n",
    "    print(\"Word {} is \\\"{}\\\"\".format(bow_corpus[0][i][0], \n",
    "                                id2word[bow_corpus[0][i][0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 0.1889822365046136), (1, 0.2834733547569204), (2, 0.2834733547569204), (3, 0.2834733547569204), (4, 0.2834733547569204), (5, 0.2834733547569204), (6, 0.2834733547569204), (7, 0.2834733547569204), (8, 0.2834733547569204), (9, 0.2834733547569204), (10, 0.2834733547569204), (11, 0.2834733547569204), (12, 0.2834733547569204)], [(13, 0.39391929857916763), (14, 0.2626128657194451), (15, 0.39391929857916763), (16, 0.39391929857916763), (17, 0.39391929857916763), (18, 0.39391929857916763), (19, 0.39391929857916763)], [(20, 0.4931969619160719), (21, 0.1643989873053573), (22, 0.4931969619160719), (23, 0.4931969619160719), (24, 0.4931969619160719)], [(0, 0.18814417367671946), (21, 0.09407208683835973), (25, 0.2822162605150792), (26, 0.2822162605150792), (27, 0.2822162605150792), (28, 0.2822162605150792), (29, 0.2822162605150792), (30, 0.2822162605150792), (31, 0.2822162605150792), (32, 0.2822162605150792), (33, 0.5644325210301584)], [(14, 0.30151134457776363), (34, 0.30151134457776363), (35, 0.45226701686664544), (36, 0.45226701686664544), (37, 0.45226701686664544), (38, 0.45226701686664544)], [(34, 0.42640143271122083), (39, 0.6396021490668313), (40, 0.6396021490668313)], [(21, 0.1643989873053573), (41, 0.4931969619160719), (42, 0.4931969619160719), (43, 0.4931969619160719), (44, 0.4931969619160719)], [(21, 0.14744195615489714), (45, 0.4423258684646914), (46, 0.4423258684646914), (47, 0.4423258684646914), (48, 0.4423258684646914), (49, 0.4423258684646914)]]\n"
     ]
    }
   ],
   "source": [
    "tfidf = gensim.models.TfidfModel(dictionary=id2word, normalize=True)\n",
    "tfidf_vectors = [tfidf[id2word.doc2bow(doc)] for doc in tokens]\n",
    "print (tfidf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, \n",
    "                                       num_topics=2, \n",
    "                                       id2word=id2word, \n",
    "                                       passes=2, \n",
    "                                       workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.047*\"you\" + 0.047*\"a\" + 0.032*\"google\" + 0.028*\"when\" + 0.028*\"in\" + 0.028*\"100\" + 0.028*\"tab\" + 0.028*\"if\" + 0.028*\"smiley\" + 0.028*\"this\"\n",
      "Topic: 1 \n",
      "Words: 0.061*\"google\" + 0.041*\"best\" + 0.040*\"cat\" + 0.027*\"chrome\" + 0.027*\"the\" + 0.027*\"squooshy\" + 0.027*\"promoter\" + 0.027*\"key\" + 0.027*\"merley\" + 0.027*\"kitten\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
